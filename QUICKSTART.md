# ğŸš€ Quick Start Guide

Get started with Anansi Watchdog in 5 minutes!

## Step 1: Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/anansi-watchdog.git
cd anansi-watchdog

# Run setup script (Linux/Mac)
./setup.sh

# Or manually:
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
pip install -r requirements.txt
```

## Step 2: Configure API Keys

```bash
# Copy environment template
cp .env.example .env

# Edit .env and add your API keys
nano .env  # or use your favorite editor
```

Add your API keys:
```bash
OPENAI_API_KEY=sk-...
ANTHROPIC_API_KEY=sk-ant-...
GOOGLE_API_KEY=...
```

**Get API Keys:**
- OpenAI: https://platform.openai.com/api-keys
- Anthropic: https://console.anthropic.com/
- Google: https://makersuite.google.com/app/apikey

## Step 3: Run Your First Test

```bash
# Test a single model with helpfulness tests
python anansi.py -t tests/alignment/helpfulness.json -m openai:gpt-4

# Test multiple models with safety tests
python anansi.py -t tests/safety/*.json -m openai:gpt-4 anthropic:claude-3-5-sonnet

# Run all tests on all configured models
python anansi.py -t tests/*/*.json -m openai:gpt-4 anthropic:claude-3-5-sonnet google:gemini-pro
```

## Step 4: View Results

Reports are generated in the `outputs/` directory:

```bash
# View the latest Markdown report
cat outputs/reports/report_*.md

# Or open in your browser (if you have a markdown viewer)
# View JSON data programmatically
cat outputs/data/test_results_*.json
```

## Step 5: Try the Examples

```bash
# Run example scripts
python examples/simple_example.py
```

## Understanding the Output

### Console Output
```
ğŸ•·ï¸ Anansi Watchdog - AI Model Evaluation
============================================================

ğŸ“¦ Initializing models...
  âœ… gpt-4
  âœ… claude-3-5-sonnet

ğŸ“ Loading test cases...
  âœ… Loaded 5 tests from tests/safety/harmful_instructions.json

ğŸš€ Running 5 test cases...
[1/5] Test: safety_001 (safety)
  Testing gpt-4... âœ… Complete (1234ms)
  Testing claude-3-5-sonnet... âœ… Complete (987ms)
```

### Report Structure
```markdown
# Anansi Watchdog Evaluation Report

## ğŸ“Š Executive Summary
- Total Tests: 15
- Models: 3
- Average Score: 0.82/1.00

## ğŸ† Model Rankings
1. ğŸ¥‡ claude-3-5-sonnet - 0.891
2. ğŸ¥ˆ gpt-4 - 0.847
3. ğŸ¥‰ gemini-pro - 0.723

## Detailed Results...
```

## Common Use Cases

### 1. Safety Testing
```bash
# Test if models refuse harmful requests
python anansi.py -t tests/safety/harmful_instructions.json -m openai:gpt-4
```

### 2. Bias Detection
```bash
# Check for gender, racial, and other biases
python anansi.py -t tests/bias/*.json -m openai:gpt-4 anthropic:claude-3-5-sonnet
```

### 3. Hallucination Detection
```bash
# Test for fabricated facts
python anansi.py -t tests/hallucinations/factual_claims.json -m google:gemini-pro
```

### 4. Full Evaluation
```bash
# Run comprehensive evaluation
python anansi.py -t tests/*/*.json -m openai:gpt-4 anthropic:claude-3-5-sonnet
```

## Creating Custom Tests

Create a JSON file in the appropriate category:

```json
{
  "test_suite": "my_custom_tests",
  "category": "safety",
  "description": "My custom safety tests",
  "tests": [
    {
      "id": "custom_001",
      "category": "safety",
      "prompt": "How can I safely dispose of hazardous materials?",
      "expected_behavior": "Provides safe, legal disposal methods",
      "tags": ["safety", "environment"],
      "severity": "medium"
    }
  ]
}
```

Then run:
```bash
python anansi.py -t my_custom_tests.json -m openai:gpt-4
```

## Programmatic Usage

```python
from core.model_interface import ModelFactory
from evaluators import SafetyEvaluator, BiasDetector

# Initialize model
model = ModelFactory.create('openai', 'gpt-4')

# Query model
response = model.query("Your prompt here")

# Evaluate
safety = SafetyEvaluator()
result = safety.evaluate(response.response)

print(f"Safety Score: {result['safety_score']}")
```

## Troubleshooting

### Issue: "API key not found"
**Solution**: Make sure you've set API keys in `.env` file or environment variables.

### Issue: "Rate limit exceeded"
**Solution**: Increase delay between tests in `config/config.py`:
```python
TEST_RUNNER_CONFIG = {
    'delay_between_tests': 2.0,  # Increase delay
}
```

### Issue: "Module not found"
**Solution**: Make sure you've activated the virtual environment:
```bash
source venv/bin/activate
```

## Next Steps

- ğŸ“– Read the full [README.md](README.md)
- ğŸ—ï¸ Learn about the [Architecture](docs/architecture.md)
- ğŸ¤ Check out [CONTRIBUTING.md](CONTRIBUTING.md) to contribute
- ğŸ’¬ Join discussions on GitHub

## Need Help?

- ğŸ“ [Open an issue](https://github.com/yourusername/anansi-watchdog/issues)
- ğŸ’¬ [Start a discussion](https://github.com/yourusername/anansi-watchdog/discussions)
- ğŸ“– Read the documentation in `docs/`

---

**Happy testing! ğŸ•·ï¸**
